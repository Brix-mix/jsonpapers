{
  "title": {
    "0": "A framework for evaluating IT security investments in a banking environment",
    "1": "Mobile Security from an Information Warfare Perspective",
    "2": "Phishing within E-Commerce: A Trust and Confidence Game",
    "3": "A Conceptual Operational Risk Model for SMEs: Impact on Organisational Information Technology",
    "4": "Towards Security Effectiveness Measurement utilizing Risk-Based Security Assurance",
    "5": "Agent-Based Host Enumeration and Vulnerability Scanning Using Dynamic Topology Information",
    "6": "The Cost of Observation for Intrusion Detection: Performance Impact of Concurrent Host Observation",
    "7": "Common Challenges Faced During the Establishment of a CSIRT",
    "8": "Secure Publish-Subscribe Mediated Virtual Organizations",
    "9": "Privacy: Aspects, Definitions and a Multi-Faceted Privacy Preservation Approach",
    "10": "The Management of Security in Cloud Computing",
    "11": "Broadband broadens scope for cyber crime in Africa",
    "12": "A Digital Forensic Readiness Framework for South African SME’s",
    "13": "Deep Packet Inspection - Fear of the Unknown",
    "14": "Towards An Information Security Framework For Service-oriented Architecture",
    "15": "Considering web services security policy compatibility",
    "16": "A Novel Protocol to Allow Revocation of Votes in a Hybrid Voting System",
    "17": "Adding digital forensic readiness to the email trace header",
    "18": "Digital Evidence Management Plan",
    "19": "Towards a framework to guide compliance with IS Security policies and Regulations in a university",
    "20": "An investigation and survey of response options for Intrusion Response Systems (IRSs)",
    "21": "Social Engineering Attack Detection Model: SEADM",
    "22": "Towards an Ethical Analysis of the W3C Web Services Architecture Model",
    "23": "On Monitoring Information Flow of Outsourced Data"
  },
  "abstracts": {
    "0": "A framework for evaluating IT security investments in a banking environment  The amount of effort that can be expended on information security depends on funds available and management decisions. Organisations therefore have to prepare an annual budget for the maintenance and improvement of their information security systems. Two of the key issues that confront IT management, when dealing with IT security investments, are how to spend the IT security budget most effectively, and how to make the case for an increase in funds to maintain and further enhance information security. The aim of this paper is to present a quantitative framework as an alternative way of analysing IT security investments in a banking environment in order to address the two issues mentioned above. A two step framework is proposed. The first step utilizes a cluster analysis (CA) technique and the second step employs a linear programming technique called data envelopment analysis (DEA). The purpose of the clustering step is to ensure that evaluations are carried out in groups of homogenous bank branches while the purpose of the DEA model is to determine which of the branches make efficient use of the IT security resources available to them. Following a brief discussion of the proposed framework and techniques used, an illustrative example, based on a well known South African financial institution, is presented",
    "1": "Mobile Security from an Information Warfare Perspective With the increasing prevalence of mobile devices, there is an increasing risk that the mobile networks may be targeted by information warfare attacks. An investigation of mobile security issues from an information warfare perspective, with emphasis on computer network warfare and electronic warfare, is presented. The paper focuses on analysing prior cases of mobile security breaches from an information warfare perspective, however previous research is also discussed. The validity of the various potential and perceived threats to mobile security is discussed. Preliminary results from current research into mobile security and information warfare are reported; initial simulation results assessing the practicality of jamming and eavesdropping on 3G signals and the responses from first round of research interviews are discussed.",
    "2": "Phishing within E-Commerce: A Trust and Confidence Game E-Commerce has been plagued with problems since its inception and this paper examines one of these problems: The lack of user trust in E-commerce created by the risk of phishing. Phishing has grown exponentially together with the expansion of the Internet. This growth and the advancement of technology has not only benefitted honest Internet users, but has enabled criminals to increase their effectiveness which has caused considerable damage to this budding area of commerce. Moreover, it has negatively impacted on both the user and online business, breaking down the trust relationship between them. In an attempt to explore this problem, the following was considered; firstly, e-commerce’s vulnerability to phishing attacks. By referring to the Common Criteria Security Model, various critical security areas within e-commerce are identified, and with that, the areas of vulnerability and weakness. Secondly, the methods and techniques used in phishing such as phishing emails, phishing websites and addresses, distributed attacks and redirected attacks as well as the data that phishers seek to obtain, is examined. Furthermore, the way to reduce the risk of phishing and in turn increase the trust between users and websites is explored. Here the importance of Trust and the Uncertainty Reduction Theory plus the fine balance between trust and control is explored. Finally, the paper presents Critical Success Factors that aid in phishing prevention and control, these being: User Authentication, Website Authentication, Email Authentication, Data Cryptography, Communication, and Active Risk Mitigation.",
    "3": "A Conceptual Operational Risk Model for SMEs: Impact on Organisational Information Technology Building on prior research related to the impact of information technology (IT) and operational risk management (OPM) in the context of SMEs, this paper proposes there is a relationship between IT operational risk management and performances of SMEs. Specifically, a model is developed showing the relationship between IT operational risks, evaluation models, principal causes of IT failure, change management requirements, characteristic(s) of business information and lastly disorganised (chaotic) state of organisation(s) will never lead to the same results of operational risk management (ORM). Conceptual and empirical literature is explained within this model. The discussions are then used to generate research propositions that represent the models which in effect provide insight on how the variables are linked. Hence, further research can prove empirically the relationships and hence provide a contribution in the area of IT operational risk with regards to SMEs",
    "4": "Towards Security Effectiveness Measurement utilizing Risk-Based Security Assurance Systematic and practical approaches to risk-driven operational security evidence help ensure the effectiveness and efficiency of security controls in business-critical applications and services. This paper introduces an enhanced methodology to develop security effectiveness metrics that can be used in connection with correctness assurance of security controls. This methodology is then applied to an example system: a Push E-mail service. The methodology is based on threat and vulnerability analysis, and parallel security requirement and system architecture decomposition",
    "5": "Agent-Based Host Enumeration and Vulnerability Scanning Using Dynamic Topology Information Edge networks in enterprise networks are increasingly complex and dynamic, raising questions about the ability to maintain a current overview of computing assets on the network and their potential vulnerability. However, to respond to ongoing or impending attacks that may propagate at high speed, it has become crucial to ensure proper and efcient reachability of all network nodes that might be at risk so as to be able to assess and, where possible, mitigate the threat. In this paper we therefore propose an agent-based semiautonomous scanning mechanism which utilizes topology information to traverse networks with minimum bandwidth usage and maximum network coverage, and hence avoiding potential service degradation in large-scale structured networks. Topology information is also used to constrain propagation to a well defined network, while intermittently active hosts and topology changes are detected by using resident reactive agents plotted throughout the mechanism gradual propagation.",
    "6": "The Cost of Observation for Intrusion Detection: Performance Impact of Concurrent Host Observation Intrusion detection relies on the ability to obtain reliable and trustworthy measurements, while adversaries will inevitably target such monitoring and security systems to prevent their detection. This has led to a number of proposals for using coprocessors as protected monitoring instances. However, such coprocessors suffer from two problems, namely the ability to perform measurements without relying on the host system and the speed at which such measurements can be performed. The availability of smart, high-performance subsystems in commodity computer systems such as graphics processing units (GPU) strongly motivates an investigation into novel ways of achieving the twin objectives of self-protected observation and monitoring systems and sufficient measurement frequency. This, however, gives rise to performance penalties imposed by memory synchronization particularly in non-uniform memory architectures (NUMA) even for the case of direct memory access (DMA) transfers. Based on prior work detailing a cost model for synchronization of memory access in such advanced architectures, we report an experimental validation of the cost model using an IEEE 1394 DMA bus mastering environment, which provides full access to the measurement target’s main memory and involves multiple bus bridges and concomitant synchronization mechanisms. We observed up to 25% performance degradation, highlighting the need for efficient sampling strategies for both, memory size and a preference for quiescent data structures for monitoring executed by off-host devices.",
    "7": "Common Challenges Faced During the Establishment of a CSIRT A CSIRT is a team of dedicated information security specialists that prepares for and responds to information security incidents. When an incident occurs, members of a CSIRT can assist its constituency in determining what happened and what actions need to be taken to remedy the situation. The establishment of a CSIRT, however, is not without certain difficulties or complications. Such a project requires sustained commitment and relies largely on a circle of international trust that needs time to develop. Without these attributes, a CSIRT establishment project can run into a number of problems that can have varying effects on the successfulness of the project. This article looks at a number of common problems faced during the establishment of a CSIRT, within the set of chronological steps.",
    "8": "Secure Publish-Subscribe Mediated Virtual Organizations Digital technologies such as publish-subscribe systems present dynamic services support for inter-organizational activities. In order for these systems to achieve usage acceptance, various security requirements have to be met by the enabling technologies. In this article, we focus on confidentiality, privacy and integrity requirements for Publishers and Subscribers in a Publish-Subscribe mediated electronic market. We consider a virtual organization architecture, in which subscribers dynamically join and leave various organizations. We review techniques previously suggested in literature for providing confidentiality, privacy and integrity requirements and then present a new solution which is based on cryptographic hashes and public-key cryptography",
    "9": "Privacy: Aspects, Definitions and a Multi-Faceted Privacy Preservation Approach There are many different definitions and understandings of the concept of privacy. Here we bring all the different aspects of privacy together and propose a comprehensive definition thereof. We also introduce the three different approaches to privacy preservation, and propose a comprehensive and multifaceted approach in order to gain from the benefits of each and maximise privacy protection. We report on the evaluation of a prototype of such a privacy protective shopping environment.",
    "10": "The Management of Security in Cloud Computing Cloud computing has elevated IT to newer limits by offering the market environment data storage and capacity with flexible scalable computing processing power to match elastic demand and supply, whilst reducing capital expenditure. However the opportunity cost of the successful implementation of Cloud computing is to effectively manage the security in the cloud applications. Security consciousness and concerns arise as soon as one begins to run applications beyond the designated firewall and move closer towards the public domain. The purpose of the paper is to provide an overall security perspective of Cloud computing with the aim to highlight the security concerns that should be properly addressed and managed to realize the full potential of Cloud computing. Gartner’s list on cloud security issues, as well the findings from the International Data Corporation enterprise panel survey based on cloud threats, will be discussed in this paper",
    "11": "Broadband broadens scope for cyber crime in Africa Africa has recently seen explosive growth in information and communication technologies, making cyber crime a reality in this part of the world. This paper investigates the possibility of another increase in cyber crime as a result of the planned increased broadband access for the African continent. Currently, Africa has limited or inadequate action and controls to protect computers and networks, making it both a target of attack as well as a medium to attack other parts of the world. Cyber space threats and trends are a reality as the shortage of IT education and the absence of African languages prevents people from acting on warnings of cyber fraud. To address this problem, people need to be made aware of the threats and trends, and the potential adverse effect it may have on them: the use of pirate copies of software and operating systems increases the threats as no security updates are installed; the lack of standardized procedures can lead to uncertainties about the effectiveness of investigating techniques. An increase in broadband access will give Internet access to more users in Africa, effectively broadening the scope for cyber crime",
    "12": "A Digital Forensic Readiness Framework for South African SME’s In this digital age, most business is conducted electronically. This contemporary paradigm creates openings for potentially harmful unanticipated information security incidents of both a criminal or civil nature, with the potential to cause considerable direct and indirect damage to smaller businesses. Electronic evidence is fundamental to the successful handling of such incidents. If an organisation does not prepare proactively for such incidents it is highly likely that important relevant digital evidence will not be available. Not being able to respond effectively could be extremely damaging to smaller companies, as they are unable to absorb losses as easily as larger organisations. In order to prepare smaller businesses for incidents of this nature, the implementation of Digital Forensic Readiness policies and procedures is necessitated. Numerous varying factors such as the perceived high cost, as well as the current lack of forensic skills, make the implementation of Digital Forensic Readiness appear difficult if not infeasible for smaller organisations. In order to solve this problem it is necessary to develop a scalable and flexible framework for the implementation of Digital Forensic Readiness based on the individual risk profile of a small to medium enterprise (SME). This paper aims to determine, from literature, the concepts of Digital Forensic Readiness and how they apply to SMEs. Based on the findings, the aspects of Digital Forensics and organisational characteristics that should be included in such a framework is highlighted..",
    "13": "Deep Packet Inspection - Fear of the Unknown Enterprise and service provider customers develop, maintain and operate network infrastructure in order to support the applications required to perform their day to day tasks. These applications have certain requirements and expectations from the infrastructure, including access to public networks, and thus rely on quality of service (QoS) controls to manage network traffic. QoS controls are used to ensure non-critical applications do not hamper the operation of critical ones, all the while providing fair access to all legitimate applications. QoS systems are increasingly being used as firewalls, filtering bad traffic and allowing good traffic to traverse the network without delay. This paper investigates the effectiveness of protocol matching within current QoS classifiers and shows that even with the most up to date classifiers, “unknown” or unidentified traffic is still prevalent on a network; a serious concern for IT network administrators. This “unknown traffic could consist of viruses, attempted exploits and other un-authorized connectivity from outside sources.",
    "14": "Towards An Information Security Framework For Service-oriented Architecture Service-oriented architectures support distributed  heterogeneous environments where business transactions occur among loosely connected services. Ensuring a secure infrastructure for this environment is challenging. There are currently various approaches to addressing information security, each with its own set of benefits and difficulties. Additionally, organisations can adopt vendor-based information security frameworks to assist them in implementing adequate information security controls. Unfortunately, there is no standard information security framework that has been adopted for service-oriented architectures. This paper analyses the information security challenges faced by service-oriented architectures. Information security components for a service-oriented architecture environment are proposed. These components were developed collectively from serviceoriented architecture design principles, the ISO/IEC 27002:2005 standard, and other service-oriented architecture governance frameworks. The information security framework can assist organisations in determining information security controls for service-oriented architectures, aligned to current ISO/IEC 27002:2005 standards.",
    "15": "Considering web services security policy compatibility For most organizations supporting business-tobusiness (B2B) web services interactions, security is a growing concern. Web services providers and consumers document their primary and alternative security policy requirements and capabilities in security policy files, defined by WS-Policy, WSSecurityPolicy and WS-Security syntax. To secure message exchanges to the satisfaction of all parties, the security requirements of both web services providers and consumers need to be satisfied. This paper investigates how mutually agreed-upon security policies can be created. An analysis of the policy intersection algorithm highlights its deficiencies for finding mutually compatible policies. The interrelated effect that security policy assertion choices have on each other is identified as an important aspect not yet considered. Over and above security policy assertions, other influence on security policy choices, which may affect the security level supported by the organization, is identified. A proposal is made on how the assertions of two security policies should be considered, in order to create a secure, mutually agreed-upon security policy that will satisfy the requirements of both parties.",
    "16": "A Novel Protocol to Allow Revocation of Votes in a Hybrid Voting System A hybrid voting system allows voters to revoke their electronic vote at the polling station. This approach is meant to provide full individual and universal verifiability without introducing the threats of vote buying or voter coercion. Such an integration of traditional and electronic voting systems requires the voters’ ability to prove whether they have already voted electronically, and if so, to show which of all the electronic votes published on the public bulletin board is theirs. This paper proposes in full cryptographic detail a novel evoting protocol that allows voters to unambiguously show and prove to voting officials at the polling station if they have cast an electronic vote. If this is the case, the voters can use their secret credentials to locate their votes on the public bulletin board without giving up the secrecy of the credentials. Remarkably, our protocol enables them to do so, even if their votes have been cast by a third party that got hold of their credentials. We thus address the hardest possible attack on a voter’s right to vote. Furthermore, unlike pure e-voting systems, our protocol allows the hybrid system to provide coercion-resistance even when voters are allowed to vote for write-in candidates. Our approach is meant to appeal to governments that aim at offering voters the choice between two channels for casting votes, rather than fully replacing their traditional paper-based voting scheme with an e-voting system.",
    "17": "Adding digital forensic readiness to the email trace header The protection strategies proposed and implemented to protect users against spam, focus on specific areas that need to be protected e.g. Anti-Spam filters that protect the user’s mailbox from bulk unsolicited email. Digital forensics is based on scientifically proven methods to collect and analyze digital information. Employing digital forensic techniques to gather and analyze email information provides a new dimension to the fight against spam. Adding digital forensic readiness to email will allow for the gathering of forensic information. The digital forensic information can be used to verify information contained in the trace header of an email. The authors propose augmentations to the receive header, that is part of the trace header, currently specified for SMTP to implement digital forensic readiness. Incorporating digital forensics, adds a level of integrity to the trace header information that can be used for other purposes e.g. creating a spam detection mechanism or tracing the origin of spam. Digital forensic information is added to the email envelope so there is no effect to the content of the email. Therefore, the content remains untouched. The authors examine the addition of digital forensic information and highlight the changes that will need to be implemented in the SMTP trace header. The authors propose the gap detection algorithm that is used to find gaps in the received-tokens of the received header. The information that is generated by the gap detection algorithm is also discussed. In conclusion, the addition of digital forensic readiness adds a level of integrity to the SMTP trace header that can be used to add a level of trust",
    "18": "Digital Evidence Management Plan The degree of the reliability, integrity, and availability of information in organizations can determine the credibility of the organization. As people and applications generate information, the information is stored in various places. It is vital for the organization to know where information is stored, what format it is, and how to access it. Not all information will be evidence but it is essential that organizations identify potential evidence proactively. Good evidence is a business enabler. Organizations require ‘good’ evidence to demonstrate due diligence with respect to good corporate and IT governance and to investigate and manage internal and external incidents. All internal and external forensic investigations hinge on ‘good’ evidence. Evidence in itself is not absolute, but is valuable when used to establish the truth about a particular incident. The paper will define digital evidence, propose a theoretical Evidence Management Plan (EMP), and briefly discuss potential benefits and constraints of the implementation of the proposed EMP",
    "19": "Towards a framework to guide compliance with IS Security policies and Regulations in a university  Compliance with computer security policies and legislation is critical to educational institutions today. Universities offer Internet services to users, store personal information of learners, staff, conference and attendees. which exposes them to potential risks and legal liabilities. Failure to ensure compliance with information security laws poses significant financial and reputation risk and may invite serious scrutiny of university activities by law enforcement bodies [24]. While universities have sought various measures to achieve compliance (e.g. self-regulations, security policies, staff/student handbooks, public relation campaigns, Web and email reminders and audits.), these have had limited success in influencing user behaviours. The rate of electronic abuse and lack of compliance with policies is simply on the rise. The August 2009 EDUCAUSE Review indicates that security remains one of the top strategic issues facing higher education institutions [2]. [20] claims that half of all personal identity breaches occur in higher education. The recording industry and motion picture associations are increasingly holding institutions liable for illegal downloading of copyright materials [11] and students have also been accused of privacy violations [8]. So, what makes compliance with policies and regulations in universities difficult and how can compliance be measured and achieved effectively? This study examines the factors that influence compliance with security policies and regulations in universities. First, some key regulations governing information security in South Africa are introduced, followed by a review of the security environment and compliance behaviours in universities. A framework aligning regulatory requirements with control standards is developed to guide compliance behaviours in universities.",
    "20": "An investigation and survey of response options for Intrusion Response Systems (IRSs) The rise of attacks and incidents need additional and distinct methods of response. This paper starts a discussion by differentiating the type of operation mode such as Intrusion Detection Systems (IDSs), Intrusion Prevention Systems (IPSs) and Intrusion Response Systems (IRSs). Using characteristics of response and attack time frame, a response model is proposed to distinguish between active and passive response options. The characteristics of response include level of operations, speed and time of response, ability to learn and ability to cooperate with other devices. This paper uses the attack time frame as a response model to show the relationship between active and passive response. Furthermore, the Response Model for Intrusion Response Systems shows some other different approaches and stages of active response. Finally, in order to investigate the most common response used by security practitioner and to justify the response model, studies involving 34 samples products from both commercial and non-commercial are analysed. As a result, this paper shows a clear distinction between the options of responses",
    "21": "Social Engineering Attack Detection Model: SEADM Social engineering is a real threat to industries in this day and age even though the severity of it is extremely downplayed. The difficulty with social engineering attacks is mostly the ability to identify them. Social engineers target call centre employees, as they are normally underpaid, under skilled workers whom have limited knowledge about the information technology infrastructure. These workers are thus easy targets for the social engineer. This paper proposes a model which can be used by these workers to detect social engineering attacks in a call centre environment. The model is a quick and effective way to determine if the requester is trying to manipulate an individual into disclosing information to which the requester does not have authorization for.",
    "22": "Towards an Ethical Analysis of the W3C Web Services Architecture Model This article explores the relevance of information ethics, the field that concerns itself with the study of ethical issues arising from the development and use of such technologies, for a specific information technology viz. Web services. In particular, the Web services architecture, as conceptualised by the W3C, is analysed using Floridi’s theory of Information Ethics (IE). Firstly, it is shown that a technology such as Web services (acting as autonomous software agents and artificial agents with moral agency) should and could be subjected to a systematic ethical analysis that yields useful results. Secondly, the suitability and applicability of Floridi’s ethical theory of IE is demonstrated by applying it to a complex system such as the Web services architecture. It is shown how the central notion of IE, viz. socalled levels of abstraction, supports major software systems design principles such as top-down design, structured analysis and design, and stepwise refinement and affords. This result is of particular significance since it opens up opportunities for the systematic and appropriate ethical analysis of any software system and may provide a general approach to “ethics by design”",
    "23": "On Monitoring Information Flow of Outsourced Data Data outsourcing is an Internet-based paradigm that allows organizations to share data cost-effectively by transferring data to a third-party service provider for management. Enforcing outsourced data privacy in untrustworthy environments is challenging because the data needs to be kept secret both from unauthorized users and the service provider (SP). Existing approaches propose that the data owner(s) encrypt the data before it is transferred to the service provider to preserve confidentiality. Access is only granted to a user initiated program if the key presented can decrypt the data into a readable format. Therefore the data owner can control access to the data without having to worry about the management costs. However, this approach fails to monitor the data once it has been retrieved from the SP’s end. So, a user can retrieve information from the SP’s end and share it with unauthorized users or even the SP. We propose a conceptual framework, based on the concept of dependence graphs, for monitoring data exchanges between programs in order to prevent unauthorized access. The framework has a distributed architecture which is suitable for data outsourcing environments and the web in general. Each data object contains a cryptographic tag (like an invisible digital watermark) that is computed by using a cryptographic hash function to combine the checksum of the data and the encryption key. In order to execute an operation with a data object the key presented for decryption must match the one associated with the user’s role and generate a cryptographic tag that matches the one embedded into the data. Tracing data exchanges, in this way, can leverage data privacy for organizations that transfer data management to third party service providers."
  }
}